{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# Refactoring Your Code to Use Pipelines\n", "\n", "## Introduction\n", "\n", "In this lesson, you will learn how to use the core features of scikit-learn pipelines to refactor existing machine learning preprocessing code into a portable pipeline format.\n", "\n", "## Objectives\n", "\n", "You will be able to:\n", "\n", "* Recall the benefits of using pipelines\n", "* Describe the difference between a `Pipeline`, a `FeatureUnion`, and a `ColumnTransformer` in scikit-learn\n", "* Iteratively refactor existing preprocessing code into a pipeline\n", "\n", "## Pipelines in the Data Science Process\n", "\n", "***If my code already works, why do I need a pipeline?***\n", "\n", "As we covered previously, pipelines are a great way to organize your code in a DRY (don't repeat yourself) fashion. It also allows you to perform cross validation (including `GridSearchCV`) in a way that avoids leakage, because you are performing all preprocessing steps separately. Finally, it's helpful if you want to deploy your code, since it means that you only need to pickle the overall pipeline, rather than pickling the fitted model as well as all of the fitted preprocessing transformers.\n", "\n", "***Then why not just write a pipeline from the start?***\n", "\n", "Pipelines are designed for efficiency rather than readability, so they can become very confusing very quickly if something goes wrong. (All of the data is in NumPy arrays, not pandas dataframes, so there are no column labels by default.)\n", "\n", "Therefore it's a good idea to write most of your code without pipelines at first, then refactor it. Eventually if you are very confident with pipelines you can save time by writing them from the start, but it's okay if you stick with the refactoring strategy!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Code without Pipelines\n", "\n", "Let's say we have the following (very-simple) dataset:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Preprocessing Steps without Pipelines\n", "\n", "These steps should be a review of preprocessing steps you have learned previously. \n", "\n", "#### One-Hot Encoding Categorical Data\n", "\n", "If we just tried to apply a `StandardScaler` then a `LogisticRegression` to this dataset, we would get a `ValueError` because the values in `category` are not yet numeric.\n", "\n", "So, let's use a `OneHotEncoder` to convert the `category` column into multiple dummy columns representing each of the categories present:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Feature Engineering\n", "\n", "Let's say for the sake of example that we wanted to add a new feature called `number_odd`, which is `1` when the value of `number` is odd and `0` when the value of `number` is even. (It's not clear why this would be useful, but you can imagine a more realistic example, e.g. a boolean flag related to a purchase threshold that triggers free shipping.)\n", "\n", "We don't want to remove `number` and replace it with `number_odd`, we want an entire new feature `number_odd` to be added.\n", "\n", "Let's make a custom transformer for this purpose. Specifically, we'll use a `FunctionTransformer` ([documentation here](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html)). As you might have guessed from the name, a `FunctionTransformer` takes in a function as an argument (similar to the `.apply` dataframe method) and uses that function to transform the data. Unlike just using `.apply`, this transformer has the typical `.fit_transform` interface and can be used just like any other transformer (including being used in a pipeline)."]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Scaling\n", "\n", "Then let's say we want to scale all of the features after the previous steps have been taken:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Bringing It All Together\n", "\n", "Here is the full preprocessing example without a pipeline:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now let's rewrite that with pipeline logic!"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Pieces of a Pipeline\n", "\n", "### `Pipeline` Class\n", "\n", "In a previous lesson, we introduced the most fundamental part of pipelines: the `Pipeline` class. This class is useful if you want to perform the same steps on every single column in your dataset. A simple example of just using a `Pipeline` would be:\n", "\n", "```python\n", "pipe = Pipeline(steps=[\n", "    (\"scaler\", StandardScaler()),\n", "    (\"model\", LogisticRegression())\n", "])\n", "```\n", "\n", "However, many interesting datasets contain a mixture of kinds of data (e.g. numeric and categorical data), which means you often do not want to perform the same steps on every column. For example, one-hot encoding is useful for converting categorical data into a format that is usable in ML models, but one-hot encoding numeric data is a bad idea. You also usually want to apply different feature engineering processes to different features.\n", "\n", "In order to apply different data cleaning and feature engineering steps to different columns, we'll use the `FeatureUnion` and `ColumnTransformer` classes."]}, {"cell_type": "markdown", "metadata": {}, "source": ["### `ColumnTransformer` Class\n", "\n", "The core idea of a `ColumnTransformer` is that you can **apply different preprocessing steps to different columns of the dataset**.\n", "\n", "Looking at the preprocessing steps above, we only want to apply the `OneHotEncoder` to the `category` column, so this is a good use case for a `ColumnTransformer`:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["The pipeline returns a NumPy array, but we can convert it back into a dataframe for readability if we want to:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Interpreting the `ColumnTransformer` Example\n", "\n", "Let's go back and look at each of those steps more closely.\n", "\n", "First, creating a column transformer. Here is what that code looked like above:\n", "\n", "```python\n", "# Create a column transformer\n", "col_transformer = ColumnTransformer(transformers=[\n", "    (\"ohe\", OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\"), [\"category\"])\n", "], remainder=\"passthrough\")\n", "```\n", "\n", "Here is the same code, spread out so we can add more comments explaining what's happening:\n", "\n", "```python\n", "# Create a column transformer\n", "col_transformer = ColumnTransformer(\n", "    # ColumnTransformer takes a list of \"transformers\", each of which\n", "    # is represented by a three-tuple (not just a transformer object)\n", "    transformers=[\n", "        # Each tuple has three parts\n", "        (\n", "            # (1) This is a string representing the name. It is there\n", "            # for readability and so you can extract information from\n", "            # the pipeline later. scikit-learn doesn't actually care\n", "            # what the name is.\n", "            \"ohe\",\n", "            # (2) This is the actual transformer\n", "            OneHotEncoder(categories=\"auto\", handle_unknown=\"ignore\"),\n", "            # (3) This is the list of columns that the transformer should\n", "            # apply to. In this case, there is only one column, but it\n", "            # still needs to be in a list\n", "            [\"category\"]\n", "        )\n", "        # If we wanted to perform multiple different transformations\n", "        # on different columns, we would add more tuples here\n", "    ],\n", "    # By default, any column that is not specified in the list of\n", "    # transformer tuples will be dropped, but we can indicate that we\n", "    # want them to stay as-is if we set remainder=\"passthrough\"\n", "    remainder=\"passthrough\"\n", ")\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Next, putting the column transformer into a pipeline. Here is that original code:\n", "\n", "```python\n", "# Create a pipeline containing the single column transformer\n", "pipe = Pipeline(steps=[\n", "    (\"col_transformer\", col_transformer)\n", "])\n", "```\n", "\n", "And again, here it is with more comments:\n", "\n", "```python\n", "# Create a pipeline containing the single column transformer\n", "pipe = Pipeline(\n", "    # Pipeline takes a list of \"steps\", each of which is\n", "    # represented by a two-tuple (not just a transformer or\n", "    # estimator object)\n", "    steps=[\n", "        # Each tuple has two parts\n", "        (\n", "            # (1) This is name of the step. Again, this is for\n", "            # readability and retrieving steps later, so just\n", "            # choose a name that makes sense to you\n", "            \"col_transformer\",\n", "            # (2) This is the actual transformer or estimator.\n", "            # Note that a transformer can be a simple one like\n", "            # StandardScaler, or a composite one like a\n", "            # ColumnTransformer (shown here), a FeatureUnion,\n", "            # or another Pipeline.\n", "            # Typically the last step will be an estimator\n", "            # (i.e. a model that makes predictions)\n", "            col_transformer\n", "        )\n", "    ]\n", ")\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### `FeatureUnion` Class\n", "\n", "A `FeatureUnion` **concatenates together the results of multiple different transformers**. While `Pipeline` and a `ColumnTransformer` are usually enough to perform basic *data cleaning* forms of preprocessing, it's also helpful to be able to use a `FeatureUnion` for *feature engineering* forms of preprocessing.\n", "\n", "Let's use a `FeatureUnion` to add on the `number_odd` feature from before. Because we only want this transformation to apply to the `number` column, we need to wrap it in a `ColumnTransformer` again. Let's call this new one `feature_eng` to indicate what it is doing:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Let's also rename the other `ColumnTransformer` to `original_features_encoded` to make it clearer what it is responsible for:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can combine those two into a `FeatureUnion`:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["And put that `FeatureUnion` into a `Pipeline`:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Again, here it is as a more-readable dataframe:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["#### Interpreting the `FeatureUnion` Example\n", "\n", "Once more, here was the code used to create the `FeatureUnion`:\n", "\n", "```python\n", "feature_union = FeatureUnion(transformer_list=[\n", "    (\"encoded_features\", original_features_encoded),\n", "    (\"engineered_features\", feature_eng)\n", "])\n", "```\n", "\n", "And here it is spread out with more comments:\n", "\n", "```python\n", "feature_union = FeatureUnion(\n", "    # FeatureUnion takes a \"transformer_list\" containing\n", "    # two-tuples (not just transformers)\n", "    transformer_list=[\n", "        # Each tuple contains two elements\n", "        (\n", "            # (1) Name of the feature. If you make this \"drop\",\n", "            # the transformer will be ignored\n", "            \"encoded_features\",\n", "            # (2) The actual transformer (in this case, a \n", "            # ColumnTransformer). This one will produce the\n", "            # numeric features as-is and the categorical\n", "            # features one-hot encoded\n", "            original_features_encoded\n", "        ),\n", "        # Here is another tuple\n", "        (\n", "            # (1) Name of the feature\n", "            \"engineered_features\",\n", "            # (2) The actual transformer (again, a\n", "            # ColumnTransformer). This one will produce just\n", "            # the flag of whether the number is even or odd\n", "            feature_eng\n", "        )\n", "    ]\n", ")\n", "```"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Adding Final Steps to Pipeline\n", "\n", "If we want to scale all of the features at the end, this doesn't require any additional `ColumnTransformer` or `FeatureUnion` objects, it just means we need to add another step in our `Pipeline` like this:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Additionally, if we want to add an estimator (model) as the last step, we can do it like this:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Complete Refactored Pipeline Example\n", "\n", "Below is the complete pipeline (without the estimator), which produces the same output as the original full preprocessing example:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Just to confirm, this produces the same result as the previous function:"]}, {"cell_type": "markdown", "metadata": {}, "source": ["We achieved the same thing in fewer lines of code, better prevention of leakage, and the ability to pickle the whole process!\n", "\n", "Note that in both cases we returned the object or objects used for preprocessing so they could be used on test data. Without a pipeline, we would need to apply each of the transformers in `transformers`. With a pipeline, we would just need to use `pipe.transform` on test data."]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Summary\n", "\n", "In this lesson, you learned how to make more-sophisticated pipelines using `ColumnTransformer` and `FeatureUnion` objects in addition to `Pipeline`s. We started with a preprocessing example that used sckit-learn code without pipelines, and rewrote it to use pipelines. Along the way we used `ColumnTransformer` to conditionally preprocess certain columns while leaving others alone, and `FeatureUnion` to combine engineered features with preprocessed versions of the original data. Now you should have a clearer idea of how pipelines can be used for non-trivial preprocessing tasks."]}], "metadata": {"kernelspec": {"display_name": "Python (learn-env)", "language": "python", "name": "learn-env"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.5"}}, "nbformat": 4, "nbformat_minor": 4}